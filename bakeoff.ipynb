{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Classification Modeling\n",
    "The goal of this week's assessment is to find the model which best predicts whether or not a person will default on their bank loan. In doing so, we want to utilize all of the different tools we have learned over the course: data cleaning, EDA, feature engineering/transformation, feature selection, hyperparameter tuning, and model evaluation. \n",
    "\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "This research aimed at the case of customers default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel Sorting Smoothing Method to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default. \n",
    "\n",
    "- NT is the abbreviation for New Taiwain. \n",
    "\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables: \n",
    "- X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "- X2: Gender (1 = male; 2 = female). \n",
    "- X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "- X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "- X5: Age (year). \n",
    "- X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: \n",
    "    - X6 = the repayment status in September, 2005; \n",
    "    - X7 = the repayment status in August, 2005; . . .;\n",
    "    - etc...\n",
    "    - X11 = the repayment status in April, 2005. \n",
    "    - The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "- X12-X17: Amount of bill statement (NT dollar). \n",
    "    - X12 = amount of bill statement in September, 2005;\n",
    "    - etc...\n",
    "    - X13 = amount of bill statement in August, 2005; . . .; \n",
    "    - X17 = amount of bill statement in April, 2005. \n",
    "- X18-X23: Amount of previous payment (NT dollar). \n",
    "    - X18 = amount paid in September, 2005; \n",
    "    - X19 = amount paid in August, 2005; . . .;\n",
    "    - etc...\n",
    "    - X23 = amount paid in April, 2005. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You will fit three different models (KNN, Logistic Regression, and Decision Tree Classifier) to predict credit card defaults and use gridsearch to find the best hyperparameters for those models. Then you will compare the performance of those three models on a test set to find the best one.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process/Expectations\n",
    "\n",
    "- You will be working in pairs for this assessment\n",
    "\n",
    "### Please have ONE notebook and be prepared to explain how you worked in your pair.\n",
    "\n",
    "1. Clean up your data set so that you can perform an EDA. \n",
    "    - This includes handling null values, categorical variables, removing unimportant columns, and removing outliers.\n",
    "2. Perform EDA to identify opportunities to create new features.\n",
    "    - [Great Example of EDA for classification](https://www.kaggle.com/stephaniestallworth/titanic-eda-classification-end-to-end) \n",
    "    - [Using Pairplots with Classification](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)\n",
    "3. Engineer new features. \n",
    "    - Create polynomial and/or interaction features. \n",
    "    - Additionaly, you must also create **at least 2 new features** that are not interactions or polynomial transformations. \n",
    "        - *For example, you can create a new dummy variable that based on the value of a continuous variable (billamount6 >2000) or take the average of some past amounts.*\n",
    "4. Perform some feature selection. \n",
    "    \n",
    "5. You must fit **three** models to your data and tune **at least 1 hyperparameter** per model. \n",
    "6. Using the F-1 Score, evaluate how well your models perform and identify your best model.\n",
    "7. Using information from your EDA process and your model(s) output provide insight as to which borrowers are more likely to deafult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/training_data.csv') #, index_col=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns using data dictionary\n",
    "rename_dict= {'Unnamed: 0':'id','X1': 'credit_given', 'X2': 'gender', 'X3': 'education',\n",
    "              'X4': 'married','X5': 'age', 'X6': 'repayment_sept', 'X7': 'repayment_aug', \n",
    "              'X8':'repayment_jul', 'X9': 'repayment_jun', 'X10': 'repayment_may', \n",
    "              'X11':'repayment_apr', 'X12': 'bill_sept', 'X13': 'bill_aug', 'X14':'bill_jul',\n",
    "              'X15': 'bill_jun', 'X16':'bill_may', 'X17':'bill_apr', 'X18': 'payment_sept',\n",
    "              'X19': 'payment_aug', 'X20':'payment_jul', 'X21': 'payment_jun',\n",
    "              'X22': 'payment_may', 'X23': 'payment_apr'}\n",
    "df=df.rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the headers row\n",
    "to_drop =df[df.id=='ID'].index\n",
    "df.drop(to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change datatypes to float\n",
    "cont_features = ['credit_given', 'age', 'bill_sept', 'bill_aug', 'bill_jul',\n",
    "       'bill_jun', 'bill_may', 'bill_apr', 'payment_sept', 'payment_aug',\n",
    "       'payment_jul', 'payment_jun', 'payment_may', 'payment_apr']\n",
    "\n",
    "df[cont_features] = df[cont_features].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to be used in the models\n",
    "X = df.drop('Y', axis = 1) # grabs everything else but target\n",
    "\n",
    "# Create target variable\n",
    "y = df['Y'].astype(int) # y is the column we're trying to predict\n",
    "\n",
    "cat_features = ['gender', 'education', 'married','repayment_sept', \n",
    "         'repayment_aug', 'repayment_jul', 'repayment_jun',\n",
    "       'repayment_may', 'repayment_apr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute outliers\n",
    "\n",
    "for feat in cont_features:\n",
    "    abv_5_std = X[feat].mean()+ 5*X[feat].std()\n",
    "    X[feat] = np.where(X[feat]>abv_5_std, X[feat].mean()+ 5*X[feat].std(), X[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(X.credit_given,y, orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.3756510789308\n",
      "35.699085123309466\n",
      "\n",
      "\n",
      "177909.01375548774\n",
      "129301.44789180589\n"
     ]
    }
   ],
   "source": [
    "print(X.age[y.values ==0].mean())\n",
    "print(X.age[y.values ==1].mean())\n",
    "print('\\n')\n",
    "print(X.credit_given[y.values ==0].mean())\n",
    "print(X.credit_given[y.values ==1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(X.age[y.values ==0]);\n",
    "# sns.distplot(X.age[y.values ==1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(X.credit_given[y.values ==0]);\n",
    "# sns.distplot(X.credit_given[y.values ==1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(X[cat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate combinations of features\n",
    "X = df.iloc[:,1:]\n",
    "interactions = list(combinations(X.columns, 2))\n",
    "interaction_dict = {}\n",
    "\n",
    "for interaction in interactions:\n",
    "    X_copy = X.copy()\n",
    "    X_copy['interact'] = X_copy[interaction[0]] * X_copy[interaction[1]]  \n",
    "    model = LinearRegression() #run model with each possible interaction\n",
    "    model.fit(X_copy, y)\n",
    "    interaction_dict[model.score(X_copy, y)] = interaction # add R-squared for each interaction to a dictionary\n",
    "sorted(interaction_dict.items(), reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomials\n",
    "# logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([dummies, X[cont_features]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.4005665722379604\n",
      "2 0.49676025917926564\n",
      "3 0.518030794165316\n",
      "4 0.5227836034318399\n",
      "5 0.5219302436693741\n",
      "6 0.5238188207731355\n",
      "7 0.5260560600078957\n",
      "8 0.0\n",
      "9 0.0\n"
     ]
    }
   ],
   "source": [
    "# Select K-Best - use loop to determine best K value, which is 7\n",
    "for k in range(1, 10):\n",
    "    selector = SelectKBest(k=k)\n",
    "    selector.fit(X, y)\n",
    "    kbest_features = X.columns[selector.get_support()]\n",
    "    logreg = LogisticRegression(C=1e5, class_weight='balanced')\n",
    "    logreg.fit(X[kbest_features], y)\n",
    "    y_pred = logreg.predict(X[kbest_features])\n",
    "    print(k, metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(k=7)\n",
    "selector.fit(X, y)\n",
    "kbest_features = X.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Fitting and Hyperparameter Tuning\n",
    "Logistic Regression, Lasso, KNN, Decision Tree, Random Forest, Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.80859589,  0.80859589, -0.02310086, ..., -0.42342121,\n",
       "        -0.42865128, -0.39298855],\n",
       "       [ 1.23671171, -1.23671171, -0.02310086, ...,  0.1580173 ,\n",
       "         0.15198046,  0.44045214],\n",
       "       [ 1.23671171, -1.23671171, -0.02310086, ..., -0.3583001 ,\n",
       "        -0.41897409, -0.39298855],\n",
       "       ...,\n",
       "       [ 1.23671171, -1.23671171, -0.02310086, ..., -0.22960837,\n",
       "        -0.13833541, -0.32381297],\n",
       "       [-0.80859589,  0.80859589, -0.02310086, ..., -0.42342121,\n",
       "        -0.13833541, -0.14295634],\n",
       "       [-0.80859589,  0.80859589, -0.02310086, ..., -0.42342121,\n",
       "        -0.42865128, -0.39298855]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3598363571682675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Khyatee/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1e5, class_weight='balanced')\n",
    "logreg.fit(X,y)\n",
    "y_pred = logreg.predict(X)\n",
    "df['logistic_preds'] = y_pred\n",
    "print('F1 Score:', metrics.f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
